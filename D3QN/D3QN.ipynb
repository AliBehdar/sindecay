{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame,random,gymnasium as gym,numpy as np,matplotlib.pyplot as plt\n",
    "import tensorflow as tf,os,warnings\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "from collections import deque\n",
    "from tensorflow.python.framework import random_seed\n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings(\"ignore\", message=\"Model's `__init__()` arguments contain non-serializable objects.\")\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "np.random.default_rng(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random_seed.set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Model):\n",
    "    def __init__(self, state_size: int, action_size: int, ):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        :param state_size: The size of the state space.\n",
    "        :param action_size: The size of the action space.\n",
    "        :param hidden_size: The size of the hidden layers.\n",
    "        \"\"\"\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.num_action = action_size\n",
    "        self.layer1 = tf.keras.layers.Dense(hidden_size, activation='relu')# Define the first hidden layer with ReLU activation\n",
    "        self.layer2 = tf.keras.layers.Dense(hidden_size, activation='relu')# Define the second hidden layer with ReLU activation\n",
    "        self.state = tf.keras.layers.Dense(self.num_action)# Define the output layer for state values\n",
    "        self.action = tf.keras.layers.Dense(self.num_action)# Define the output layer for action values\n",
    "\n",
    "    def call(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        :param state: Input state.\n",
    "        :return: Value function Q(s, a).\n",
    "        \"\"\"\n",
    "        layer1 = self.layer1(state) # Pass the input state through the first hidden layer      \n",
    "        layer2 = self.layer2(layer1)  # Pass the result through the second hidden layer\n",
    "        state = self.state(layer2) # Compute the state values       \n",
    "        action = self.action(layer2) # Compute the action values        \n",
    "        mean = tf.keras.backend.mean(action, keepdims=True)# Calculate the mean of the action values \n",
    "        advantage = (action - mean)# Calculate the advantage by subtracting the mean action value      \n",
    "        value = state + advantage # Compute the final Q-values by adding state values and advantages \n",
    "\n",
    "        return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2/300, score: -281.3236531758891, e: 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Passed in object <KerasVariable shape=(8, 86), dtype=float32, path=network_4/dense_16/kernel> of type 'Variable', not tf.Tensor or tf.Variable or ExtensionType.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 258\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# if training is ready\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (update_cnt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# UPDATING THE Q-VALUE\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m#agent.update_Gamma()\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# if hard update is needed\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_cnt \u001b[38;5;241m%\u001b[39m agent\u001b[38;5;241m.\u001b[39mtarget_update \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[5], line 79\u001b[0m, in \u001b[0;36mDQNAgent.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m dqn_variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn\u001b[38;5;241m.\u001b[39mtrainable_variables\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 79\u001b[0m     \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn_variable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     states      \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(np\u001b[38;5;241m.\u001b[39mvstack(states), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     82\u001b[0m     actions     \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(actions, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:873\u001b[0m, in \u001b[0;36mGradientTape.watch\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor):\n\u001b[0;32m    865\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Ensures that `tensor` is being traced by this tape.\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m    ValueError: if it encounters something that is not a tensor.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m _extract_tensors_and_variables(tensor):\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backprop_util\u001b[38;5;241m.\u001b[39mIsTrainable(t):\n\u001b[0;32m    875\u001b[0m       logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[0;32m    876\u001b[0m           logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dtype of the watched tensor must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    877\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating (e.g. tf.float32), got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m, t\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:700\u001b[0m, in \u001b[0;36m_extract_tensors_and_variables\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[38;5;28;01myield from\u001b[39;00m _extract_tensors_and_variables(components)\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 700\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed in object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    701\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, not tf.Tensor or tf.Variable or ExtensionType.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Passed in object <KerasVariable shape=(8, 86), dtype=float32, path=network_4/dense_16/kernel> of type 'Variable', not tf.Tensor or tf.Variable or ExtensionType."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_history=[]\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            epsilon_decay (float): step size to decrease epsilon\n",
    "            lr (float): learning rate\n",
    "            max_epsilon (float): max value of epsilon\n",
    "            min_epsilon (float): min value of epsilon\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # CREATING THE Q-Network\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.action_space.seed(seed)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        # hyper parameters\n",
    "        memory_size = 100000\n",
    "        self.lr = 0.001\n",
    "        self.target_update = target_update\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.dqn = Network(self.state_size, self.action_size)\n",
    "        self.dqn_target = Network(self.state_size, self.action_size)\n",
    "        self.train_start = 1000\n",
    "\n",
    "        self.optimizers = optimizers.Adam(learning_rate=self.lr, )\n",
    "        \n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.Soft_Update = False # use soft parameter update\n",
    "\n",
    "        self.TAU = 0.1 # target network soft update hyperparameter\n",
    "        \n",
    "        self._target_hard_update()\n",
    "        \n",
    "    # EXPLORATION VS EXPLOITATION\n",
    "    def get_action(self, state, epsilon):\n",
    "        q_value = self.dqn(tf.convert_to_tensor([state], dtype=tf.float32))[0]\n",
    "        # Choose an action a in the current world state (s)\n",
    "        # If this number < greater than epsilon doing a random choice --> exploration\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.choice(self.action_size)\n",
    "\n",
    "        ## Else --> exploitation (taking the biggest Q value for this state)\n",
    "        else:\n",
    "            action = np.argmax(q_value) \n",
    "\n",
    "        return action\n",
    "    \n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # UPDATING THE Q-VALUE\n",
    "    def train_step(self):\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states      = [i[0] for i in mini_batch]\n",
    "        actions     = [i[1] for i in mini_batch]\n",
    "        rewards     = [i[2] for i in mini_batch]\n",
    "        next_states = [i[3] for i in mini_batch]\n",
    "        dones       = [i[4] for i in mini_batch]\n",
    "        \n",
    "        dqn_variable = self.dqn.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(dqn_variable)\n",
    "            \n",
    "            states      = tf.convert_to_tensor(np.vstack(states), dtype=tf.float32)\n",
    "            actions     = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "            rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(np.vstack(next_states), dtype=tf.float32)\n",
    "            dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "            \n",
    "            next_Qs = self.dqn(next_states)\n",
    "            next_Qs = tf.stop_gradient(next_Qs)\n",
    "            next_Q_targs = self.dqn_target(next_states)\n",
    "            next_action = tf.argmax(next_Qs, axis=1)\n",
    "            target_value = tf.reduce_sum(tf.one_hot(next_action, self.action_size) * next_Q_targs, axis=1)\n",
    "            \n",
    "            mask = 1 - dones\n",
    "            target_value = rewards + self.gamma * target_value * mask \n",
    "            \n",
    "            curr_Qs = self.dqn(states)\n",
    "            \n",
    "            main_value = tf.reduce_sum(tf.one_hot(actions, self.action_size) * curr_Qs, axis=1)\n",
    "            error = tf.square(main_value - target_value) * 0.5\n",
    "            loss  = tf.reduce_mean(error)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "        dqn_grads = tape.gradient(loss, dqn_variable)\n",
    "        self.optimizers.apply_gradients(zip(dqn_grads, dqn_variable))\n",
    "        \n",
    "    # after some time interval update the target model to be same with model\n",
    "    def _target_hard_update(self):\n",
    "        if not self.Soft_Update:\n",
    "            self.dqn_target.set_weights(self.dqn.get_weights())\n",
    "            return\n",
    "        if self.Soft_Update:\n",
    "            q_model_theta = self.dqn.get_weights()\n",
    "            dqn_target_theta = self.dqn_target.get_weights()\n",
    "            counter = 0\n",
    "            for q_weight, target_weight in zip(q_model_theta, dqn_target_theta):\n",
    "                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n",
    "                dqn_target_theta[counter] = target_weight\n",
    "                counter += 1\n",
    "            self.dqn_target.set_weights(dqn_target_theta)\n",
    "    \n",
    "    def update_Gamma(self):\n",
    "        self.gamma = 1 - 0.985 * (1 - self.gamma)\n",
    "    def load(self, phat):\n",
    "        \n",
    "        self.dqn = tf.keras.models.load_model(phat, custom_objects={'Network': Network})\n",
    "    def save(self, phat):\n",
    "        self.dqn.save(phat)\n",
    "\n",
    "Train=True\n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\",render_mode=\"human\" if not Train else None)\n",
    "\n",
    "# parameters\n",
    "target_update = 20\n",
    "\n",
    "\n",
    "# INITIALIZING THE Q-PARAMETERS\n",
    "hidden_size = 86\n",
    "max_episodes = 300  # Set total number of episodes to train agent on.\n",
    "batch_size =128#128\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.025            # Exponential decay rate for exploration prob\n",
    "\n",
    "# train\n",
    "agent = DQNAgent(\n",
    "    env, \n",
    "#     memory_size, \n",
    "    batch_size, \n",
    "    target_update, \n",
    "#     epsilon_decay,\n",
    ")\n",
    "save_path='./weights-and-plot/final_weights'\n",
    "load_path='./weights-and-plot/final_weights' + '_' + '300'\n",
    "def plot_training(episode):\n",
    "        agent.save(save_path + '_' + f'{episode}')\n",
    "        print('\\n~~~~~~Interval Save: Model saved.\\n')\n",
    "        sma_reward = np.convolve(reward_history, np.ones(50)/50, mode='valid')\n",
    "        max_reward=np.max(reward_history)\n",
    "        min_reward=np.min(reward_history)\n",
    "        #normalized_loss = np.interp(loss_history, (np.min(loss_history), np.max(loss_history)), (min_reward/2,max_reward))\n",
    "        normalized_epsilon = np.interp(epsilon_history, (np.min(epsilon_history), np.max(epsilon_history)), (min_reward/4,max_reward))\n",
    "        plt.plot(loss_history, label='Loss', color='#CB291A', alpha=0.8)\n",
    "        \n",
    "        plt.title(\"Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"loss\")\n",
    "\n",
    "        plt.xlim(0, len(loss_history))\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if episode == max_episodes:\n",
    "            plt.savefig('./weights-and-plot/loss-of-training_progress.png', format='png', dpi=600, bbox_inches='tight')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        #Plot Rewards,SMA 50 Reward ,Normalized Loss and Normalized Epsilon\n",
    "        plt.plot(reward_history, label='Raw Reward', color='#F6CE3B', alpha=0.8)\n",
    "\n",
    "        plt.plot(sma_reward, label='SMA 50 Reward', color='#385DAA')\n",
    "\n",
    "        plt.plot(normalized_epsilon, label='Normalized Epsilon', color='green', alpha=0.8)\n",
    "        \n",
    "        #plt.plot(normalized_loss, label='Normalized Loss', color='#CB291A', alpha=0.8)\n",
    "        \n",
    "        plt.title(\"Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save as file if last episode\n",
    "        if episode == max_episodes:\n",
    "            plt.savefig('./weights-and-plot/training_progress.png', format='png', dpi=600, bbox_inches='tight')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    save_intervalve=100\n",
    "    if Train:\n",
    "\n",
    "        update_cnt    = 0\n",
    "        # TRAINING LOOP\n",
    "        #List to contain all the rewards of all the episodes given to the agent\n",
    "        scores = []\n",
    "        reward_history=[]\n",
    "        epsilon_history=[]\n",
    "        # EACH EPISODE    \n",
    "        for episode in range(1,max_episodes+1):\n",
    "            ## Reset environment and get first new observation\n",
    "            state = agent.env.reset(seed=1)\n",
    "            state=state[0]\n",
    "            episode_reward = 0\n",
    "            done = False  # has the enviroment finished?\n",
    "            \n",
    "                \n",
    "            # EACH TIME STEP    \n",
    "            while not done :\n",
    "            # for step in range(max_steps):  # step index, maximum step is 200\n",
    "                update_cnt += 1\n",
    "                # EXPLORATION VS EXPLOITATION\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                action = agent.get_action(state, epsilon)\n",
    "            \n",
    "                # TAKING ACTION\n",
    "                next_state, reward, done, _ ,_= agent.env.step(action)\n",
    "                if isinstance(state, tuple): \n",
    "                        next_state = next_state[0]\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Our new state is state\n",
    "                state = next_state\n",
    "\n",
    "                episode_reward += reward\n",
    "\n",
    "                # if episode ends\n",
    "                if done:\n",
    "                    scores.append(episode_reward)\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.4}\".format(episode+1, max_episodes, episode_reward, epsilon,)) \n",
    "                    break\n",
    "                # if training is ready\n",
    "                if (update_cnt >= agent.batch_size):\n",
    "                    # UPDATING THE Q-VALUE\n",
    "                    agent.train_step()\n",
    "                    #agent.update_Gamma()\n",
    "                \n",
    "                    # if hard update is needed\n",
    "                    if update_cnt % agent.target_update == 0:\n",
    "                        agent._target_hard_update()\n",
    "            \n",
    "            reward_history.append(episode_reward)   \n",
    "            epsilon_history.append(epsilon)\n",
    "            epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "            \n",
    "            if episode % save_intervalve==0:\n",
    "                plot_training(episode)\n",
    "\n",
    "    else:\n",
    "        agent.load(load_path)\n",
    "        scores = []\n",
    "        for episode in range(5):\n",
    "            state = agent.env.reset(seed=1)\n",
    "            state=state[0]\n",
    "            episode_reward = 0\n",
    "            done = False  \n",
    "            while not done:\n",
    "                action = agent.get_action(state,0.01)\n",
    "                next_state, reward, done, _ ,_= agent.env.step(action)\n",
    "                if isinstance(state, tuple): \n",
    "                        next_state = next_state[0]\n",
    "                agent.append_sample(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                if done:\n",
    "                    scores.append(episode_reward)\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.4}\".format(episode+1, max_episodes, episode_reward, 0.01)) \n",
    "                    break\n",
    "        pygame.quit()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
